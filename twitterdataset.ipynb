{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJzbTU8L4Hsx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fc812df-8bdd-480d-ff4f-5e192020ec23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark py4j\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uk7UwOh2QD_q"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, udf, regexp_replace, when\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import StringType,IntegerType,FloatType,DoubleType, ArrayType\n",
        "from pyspark.ml.classification import LogisticRegression, NaiveBayes\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.ml.linalg import VectorUDT\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pickle import FALSE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6mXGktpa-xz"
      },
      "source": [
        "Building a spark session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bb254XooQHXw"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Twitter Sentiment Analysis\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiK-HaPYbFvT"
      },
      "source": [
        "Loading in the data and creating titles for each column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w52--A0OQTf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "outputId": "a06a2b49-84ba-4c9f-c0e0-c1bf7ab15b9b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/content/training.1600000.processed.noemoticon.csv.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-e50beec5542f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/training.1600000.processed.noemoticon.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcolumn_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'flag'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'username'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ISO-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"double\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/content/training.1600000.processed.noemoticon.csv."
          ]
        }
      ],
      "source": [
        "path = \"/content/training.1600000.processed.noemoticon.csv\"\n",
        "column_names = ['target', 'id', 'date', 'flag', 'username', 'tweet']\n",
        "data = spark.read.csv(path, inferSchema=True, header=False, encoding=\"ISO-8859-1\")\n",
        "data = data.toDF(*column_names)\n",
        "data = data.withColumn(\"target\", col(\"target\").cast(\"double\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9ZChMVrbMd7"
      },
      "source": [
        "Preprocessing the data by removing anything not containing a-z uppercase or lower case\n",
        "\n",
        "*   split the sentence into words\n",
        "*   removes stopwords\n",
        "*   words to lowercase\n",
        "*   words to base(running to run)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
        "def preprocess_text(content):\n",
        "    content = re.sub('[^a-zA-Z]', ' ', content).lower().split()\n",
        "    stemmed_words = [word for word in content if word not in stop_words]\n",
        "    return ' '.join(stemmed_words)\n",
        "\n",
        "preprocess_udf = udf(preprocess_text, StringType())"
      ],
      "metadata": {
        "id": "m7egf3T8ny94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DsnHo5ybahH"
      },
      "source": [
        "Changing target value of 4 to 1\n",
        "Applying preprocess to dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfdZoXygQlIE"
      },
      "outputs": [],
      "source": [
        "data = data.withColumn(\"target\", when(col(\"target\") == 4, 1).otherwise(col(\"target\")))\n",
        "data = data.withColumn(\"clean_tweet\", preprocess_udf(col(\"tweet\")))\n",
        "data.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHmw0Lp6bl7Q"
      },
      "source": [
        "Graphing the Target counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5zmEISRQtrH"
      },
      "outputs": [],
      "source": [
        "negative_count = data.filter(col(\"target\") == 0).count()\n",
        "positive_count = data.filter(col(\"target\") == 1).count()\n",
        "print(\"Negative count:\", negative_count)\n",
        "print(\"Positive count:\", positive_count)\n",
        "\n",
        "# Plot distribution of negative and positive tweets\n",
        "plt.bar([\"Negative\", \"Positive\"], [negative_count, positive_count])\n",
        "plt.title(\"Distribution of Negative and Positive Tweets\")\n",
        "plt.xlabel(\"Sentiment\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_KhDQO0U5R"
      },
      "source": [
        "Calculating Postive and Negative Tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHrvKRyOb-S1"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import udf, col\n",
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "# Read positive and negative words into DataFrames\n",
        "positive_words = spark.read.text(\"/content/positive-words.txt\")\n",
        "negative_words = spark.read.text(\"/content/negative-words.txt\")\n",
        "\n",
        "positive_words_list = positive_words.rdd.map(lambda row: row[0]).collect()\n",
        "negative_words_list = negative_words.rdd.map(lambda row: row[0]).collect()\n",
        "\n",
        "def calculate_sentiment(clean_tweet):\n",
        "    words = clean_tweet.split()\n",
        "    positive_count = sum(1 for word in words if word in positive_words_list)\n",
        "    negative_count = sum(1 for word in words if word in negative_words_list)\n",
        "    total_count = len(words)\n",
        "    if total_count == 0:\n",
        "        return 0.5 #neutral sentiment\n",
        "    positive_prob = positive_count / total_count\n",
        "    negative_prob = negative_count / total_count\n",
        "    if positive_prob > negative_prob:\n",
        "        return 1.0\n",
        "    else:\n",
        "        return 0.0\n",
        "\n",
        "sentiment_udf = udf(calculate_sentiment, DoubleType())\n",
        "\n",
        "data = data.withColumn(\"sentiment\", sentiment_udf(col(\"clean_tweet\")))\n",
        "data.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCPorn_YbsS_"
      },
      "source": [
        "Applying Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import CountVectorizer\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml import Pipeline"
      ],
      "metadata": {
        "id": "CGWS92W5s9el"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying Logistic Regression Model"
      ],
      "metadata": {
        "id": "sqmFRLr-dLCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vecAssembler = VectorAssembler(inputCols=['sentiment'], outputCol=\"features\")\n",
        "data = vecAssembler.transform(data)\n",
        "train, test = data.randomSplit([0.7, 0.3], seed=2030)\n",
        "\n",
        "logistic_regression = LogisticRegression(labelCol=\"target\", featuresCol=\"features\")\n",
        "model_lr = logistic_regression.fit(train)\n",
        "\n",
        "predictions_lr = model_lr.transform(test)\n",
        "\n",
        "evaluator_lr = BinaryClassificationEvaluator(labelCol=\"target\", rawPredictionCol=\"prediction\")\n",
        "accuracy_lr = evaluator_lr.evaluate(predictions_lr)\n",
        "print(\"Logistic Regression Test Area Under ROC:\", accuracy_lr)"
      ],
      "metadata": {
        "id": "0q4ug0jx-dso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCzcrzIpbu7f"
      },
      "source": [
        "Applying the Naive Bayes Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Naive Bayes model\n",
        "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\", labelCol=\"target\")\n",
        "model_nb = nb.fit(train)\n",
        "predictions_nb = model_nb.transform(test)\n",
        "evaluator_nb = BinaryClassificationEvaluator(labelCol=\"target\", rawPredictionCol=\"prediction\")\n",
        "accuracy_nb = evaluator_nb.evaluate(predictions_nb)\n",
        "print(\"Naive Bayes Test Area Under ROC:\", accuracy_nb)"
      ],
      "metadata": {
        "id": "BfKehi2JoRIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dayrLkpbyeM"
      },
      "source": [
        "Creating own tweets to predict sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2gQCmYp4Auu"
      },
      "outputs": [],
      "source": [
        "new_tweets_data = [\n",
        "    (\"I hate him\", 0),\n",
        "    (\"You wouldn't believe what he said to me\", 0),\n",
        "    (\"Two scoop kinda day.\", 1),\n",
        "    (\"I cannot decide if I like or hate this product.\", 0),\n",
        "    (\"Yes\", 1)\n",
        "]\n",
        "\n",
        "new_tweets_df = spark.createDataFrame(new_tweets_data, [\"tweet\", \"sentiment\"])\n",
        "\n",
        "new_tweets_df = new_tweets_df.withColumn(\"clean_tweet\", preprocess_udf(col(\"tweet\")))\n",
        "\n",
        "new_tweets_df = vecAssembler.transform(new_tweets_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two models predicted created tweets"
      ],
      "metadata": {
        "id": "OZ68yrWttGps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_new_tweets_lr = model_lr.transform(new_tweets_df)\n",
        "predictions_new_tweets_nb = model_nb.transform(new_tweets_df)\n",
        "\n",
        "print(\"Predictions for new tweets using Logistic Regression model:\")\n",
        "predictions_new_tweets_lr.select(\"tweet\", \"prediction\").show()\n",
        "\n",
        "print(\"Predictions for new tweets using Naive Bayes model:\")\n",
        "predictions_new_tweets_nb.select(\"tweet\", \"prediction\").show()"
      ],
      "metadata": {
        "id": "aVuTJuswsLx_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}